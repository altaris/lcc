{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a19028a-042e-40d9-908a-9972dbc89a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d571ac-612c-4b3a-a081-5bccf1f5e690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bokeh.layouts as bkl\n",
    "import bokeh.plotting as bk\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "from nlnas.plotting import export_png\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "import sys\n",
    "\n",
    "from loguru import logger as logging\n",
    "\n",
    "logging.remove()\n",
    "logging.add(sys.stderr, level=\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb65f47d-43ab-4104-acd6-da423fbda699",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_MODEL_NAME = \"timm/mobilenetv3_small_050.lamb_in1k\"\n",
    "HEAD_NAME = \"model.classifier\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "085b3422-ba42-4c72-b14c-9f2023a2ab6c",
   "metadata": {},
   "source": [
    "HF_MODEL_NAME = \"microsoft/resnet-18\"\n",
    "HEAD_NAME = \"model.classifier.1\"\n",
    "\n",
    "HF_MODEL_NAME = \"google/mobilenet_v2_1.0_224\"\n",
    "HEAD_NAME = \"model.classifier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81c12b1-d1b9-4528-b52c-0e00f1bf1a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_DATASET_NAME = \"cifar100\"  # Name in Hugging Face's dataset index\n",
    "\n",
    "TRAIN_SPLIT = \"train[:80%]\"  # See HF dataset page for split name\n",
    "VAL_SPLIT = \"train[80%:]\"  # See HF dataset page for split name\n",
    "TEST_SPLIT = \"test\"  # See HF dataset page for split name\n",
    "IMAGE_KEY = \"img\"  # See HF dataset page for name of dataset column\n",
    "LABEL_KEY = \"fine_label\"  # See HF dataset page for name of dataset column"
   ]
  },
  {
   "cell_type": "raw",
   "id": "91bd9fd6-f26b-4302-9900-83370109543a",
   "metadata": {},
   "source": [
    "HF_DATASET_NAME = \"ILSVRC/imagenet-1k\"  # Name in Hugging Face's dataset index\n",
    "\n",
    "TRAIN_SPLIT = \"train[:80%]\"  # See HF dataset page for split name\n",
    "VAL_SPLIT = \"train[80%:]\"  # See HF dataset page for split name\n",
    "TEST_SPLIT = \"validation\"  # See HF dataset page for split name\n",
    "IMAGE_KEY = \"image\"  # See HF dataset page for name of dataset column\n",
    "LABEL_KEY = \"label\"  # See HF dataset page for name of dataset column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fbb711-2017-4b93-9125-d981c142ae1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filesystem-friendly names\n",
    "DATASET_NAME = HF_DATASET_NAME.replace(\"/\", \"-\")\n",
    "MODEL_NAME = HF_MODEL_NAME.replace(\"/\", \"-\")\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "LC_PATH = Path(\"out\") / \"ft\" / DATASET_NAME / MODEL_NAME / \"lc\"\n",
    "assert LC_PATH.is_dir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2996ce85-99bb-485f-a4ac-1e064e24a4ef",
   "metadata": {},
   "source": [
    "# Dataset loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda5d9a9-07c1-401f-8c4a-5a0f15d50323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlnas import HuggingFaceClassifier, HuggingFaceDataset, TimmClassifier\n",
    "\n",
    "if HF_MODEL_NAME.startswith(\"timm/\"):\n",
    "    classifier_cls = TimmClassifier\n",
    "else:\n",
    "    classifier_cls = HuggingFaceClassifier\n",
    "\n",
    "dataset = HuggingFaceDataset(\n",
    "    HF_DATASET_NAME,\n",
    "    fit_split=TRAIN_SPLIT,\n",
    "    val_split=VAL_SPLIT,\n",
    "    test_split=TEST_SPLIT,\n",
    "    predict_split=TRAIN_SPLIT,  # not a typo\n",
    "    label_key=LABEL_KEY,\n",
    "    image_processor=classifier_cls.get_image_processor(HF_MODEL_NAME),\n",
    ")\n",
    "\n",
    "y_true = dataset.y_true(\"train\").numpy()\n",
    "y_true.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4f77b3-42e6-4839-b9ee-8d8f78fce732",
   "metadata": {},
   "source": [
    "# Latent representation loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3f1286-fb16-4b7a-a7f5-7b84fd5f149b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import turbo_broccoli as tb\n",
    "\n",
    "matching_data = tb.load(LC_PATH / \"louvain\" / \"data.json\")\n",
    "y_clst = {sm: y for sm, (y, _) in matching_data.items()}\n",
    "matching = {sm: m for sm, (_, m) in matching_data.items()}\n",
    "\n",
    "list(matching_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bba719-abfc-41ad-8794-a1f3bd7512d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Latent spaces to process\n",
    "SUBMODULES = list(matching_data.keys())\n",
    "# SUBMODULES = [HEAD_NAME]\n",
    "\n",
    "# Need the latent embeddings of the last submodule, aka the output logits\n",
    "assert HEAD_NAME in SUBMODULES\n",
    "\n",
    "# Subset of classes to consider in case the true number of classes is too large\n",
    "# CLASSES = np.array(range(100))\n",
    "CLASSES = np.arange(dataset.n_classes())\n",
    "\n",
    "class_mask = np.isin(y_true, CLASSES)\n",
    "\n",
    "y_true = y_true[class_mask]\n",
    "y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1f15cd-c76b-4728-9adc-9372b7dbaab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from nlnas.utils import load_tensor_batched\n",
    "\n",
    "latent_embeddings = {}\n",
    "for sm in tqdm(SUBMODULES):\n",
    "    u = load_tensor_batched(\n",
    "        LC_PATH / \"embeddings\" / \"train\",\n",
    "        prefix=sm,\n",
    "        mask=class_mask,\n",
    "        tqdm_style=\"notebook\",\n",
    "    )\n",
    "    u = u.numpy()\n",
    "    u = u.reshape(len(u), -1)\n",
    "    u = StandardScaler().fit_transform(u)\n",
    "    latent_embeddings[sm] = u\n",
    "\n",
    "for sm, u in latent_embeddings.items():\n",
    "    print(sm, u.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf36d52d-6222-4b3e-9ba0-419fbf8fd82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = latent_embeddings[HEAD_NAME]\n",
    "y_pred = logits.argmax(axis=-1)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8ce571-4656-4b66-ae68-b7ee55a48e15",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117db61c-0166-47dc-a2e0-1fdbafddd811",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print(\"Accuracy score:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea086ed9-b85b-415e-8794-ae955ba04cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix as _confusion_matrix\n",
    "\n",
    "confusion_matrix = _confusion_matrix(y_true=y_true, y_pred=y_pred)\n",
    "confusion_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e12dad-ef53-4ff1-888a-d3e0de02e501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "size = 25\n",
    "plt.rcParams[\"figure.figsize\"] = (size, size)\n",
    "\n",
    "off_diag_cm = confusion_matrix * (1 - np.eye(len(confusion_matrix)))\n",
    "cmd = ConfusionMatrixDisplay(off_diag_cm)\n",
    "cmd.plot(include_values=False, colorbar=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616e6858-48f9-46b7-a1cf-fc12e6d8a5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = confusion_matrix * (\n",
    "    1 - np.eye(len(confusion_matrix))\n",
    ")  # Remove the diagonal\n",
    "idx = m.argsort(axis=None)  # Flat indices\n",
    "idx = np.flip(idx)\n",
    "confusion_pairs = np.unravel_index(idx, confusion_matrix.shape)\n",
    "confusion_pairs = np.stack(confusion_pairs).T\n",
    "confusion_pairs = np.array(\n",
    "    [[i, j] for i, j in confusion_pairs if confusion_matrix[i, j] > 0]\n",
    ")\n",
    "\n",
    "# confusion_pairs (M, 2): coordinates of non-diagonal strictly positive entries in\n",
    "# confusion_matrix in decreasing order. So if i, j = confusion_pairs[k], then\n",
    "# confusion_matrix[i, j] is the number of samples in true class i classified in j.\n",
    "# Furthermore, i != j and confusion_matrix[i, j] > 0.\n",
    "\n",
    "print(len(confusion_pairs), \"confusion pairs\")\n",
    "print(\"Top confusions:\")\n",
    "for i in range(5):\n",
    "    a, b = confusion_pairs[i]\n",
    "    c = confusion_matrix[a, b]\n",
    "    print(f\"- y_true={a:>3}, y_pred={b:>3}, n_err={c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83f94ad-179f-473d-9370-34fd02a9a7ac",
   "metadata": {},
   "source": [
    "# Distance distributions (true classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c72eb51-1618-40ca-9f94-29ca30b6e5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bokeh.layouts as bkl\n",
    "\n",
    "# TEST: The blue and grey line should roughly match\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "from nlnas.analysis.dd import distance_distribution, distance_distribution_plot\n",
    "\n",
    "_n, _figs = 1024, []\n",
    "for _nd in [1, 2, 8, 512, 2048]:\n",
    "    _a = np.random.randn(_n, _nd)\n",
    "    _b = pdist(_a, metric=\"euclidean\") / np.sqrt(_nd)\n",
    "    h, e = np.histogram(_b, bins=500)\n",
    "    _fig = distance_distribution_plot(h, e, _nd, height=100)\n",
    "    _fig.title = f\"Test: n={_n}, n_dim={_nd}\"\n",
    "    _figs.append(_fig)\n",
    "bk.show(bkl.row(_figs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613e49cc-ece5-46ad-b528-a8b12b0919c8",
   "metadata": {},
   "source": [
    "## Full\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33835aa3-a241-464f-895d-2cd6c0a19c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate the dataset to only consider DD_N_SAMPLES samples for distance\n",
    "# distribution computations\n",
    "DD_N_SAMPLES = 2048\n",
    "\n",
    "# For DD histograms\n",
    "RESOLUTION = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaac7de5-74e0-429d-8c88-c211ebe909db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full dataset distance distribution (DD) data\n",
    "# Each entry in the dict is itself a dict with three entries:\n",
    "# - `d`: the pdist distance matrix (it's actually just a flat vector but whatever)\n",
    "# - `hist` (RESOLUTION,): histogram counts\n",
    "# - `edges` (RESOLUTION + 1,): histogram bin edges\n",
    "\n",
    "import numpy as np\n",
    "import turbo_broccoli as tb\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "full_ds_dd = {}\n",
    "for sm, u in tqdm(latent_embeddings.items()):\n",
    "    g = tb.GuardedBlockHandler(\n",
    "        LC_PATH / \"pdist\" / \"train\" / \"full\" / (sm + \".st\")\n",
    "    )\n",
    "    for _ in g.guard():\n",
    "        h, e = distance_distribution(u[:DD_N_SAMPLES])\n",
    "        g.result = {\"hist\": h, \"edges\": e}\n",
    "    full_ds_dd[sm] = g.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436733ce-54c2-46e6-a2f5-32d61b508ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bokeh.layouts as bkl\n",
    "\n",
    "SIZE = 250\n",
    "\n",
    "rows = []\n",
    "for sm, dat in tqdm(full_ds_dd.items()):\n",
    "    h, e, u = dat[\"hist\"], dat[\"edges\"], latent_embeddings[sm]\n",
    "    figure = distance_distribution_plot(h, e, u.shape[-1], height=SIZE)\n",
    "    figure.title = (\n",
    "        \"pdist distribution (black) vs. expected χ (grey)\\n\"\n",
    "        f\"sm={sm}, n={DD_N_SAMPLES}, res={RESOLUTION}\"\n",
    "    )\n",
    "    rows.append(figure)\n",
    "\n",
    "figure = bkl.column(rows)\n",
    "bk.show(figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97ac6c2-336e-4a05-8db7-8ed8504ebc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_png(figure, filename=LC_PATH / \"full_ds_dd.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f37dae-4a58-42bc-bdea-a7621f2a45d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d393b21f-aab1-4a90-a832-92b80e7f31af",
   "metadata": {},
   "source": [
    "## Per-class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4891cc-87dc-4be1-8aef-5e8db72f82df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intra_class_dd is a two level dict that maps\n",
    "# submodule_name → true class number → class distance distribution\n",
    "# where the class distance distribution data is compliled in a dict with three keys:\n",
    "# - `d`: the pdist distance matrix (it's actually just a flat vector but whatever)\n",
    "# - `hist` (RESOLUTION,): histogram counts\n",
    "# - `edges` (RESOLUTION + 1,): histogram bin edges\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "intra_class_dd = defaultdict(dict)\n",
    "for sm, u in tqdm(latent_embeddings.items()):\n",
    "    for i in tqdm(CLASSES[:20], leave=False):\n",
    "        g = tb.GuardedBlockHandler(\n",
    "            LC_PATH / \"pdist\" / \"train\" / \"intra-class\" / str(i) / (sm + \".st\")\n",
    "        )\n",
    "        for _ in g.guard():\n",
    "            h, e = distance_distribution(u[y_true == i][:DD_N_SAMPLES])\n",
    "            g.result = {\"hist\": h, \"edges\": e}\n",
    "        intra_class_dd[sm][i] = g.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93424203-9650-4045-a574-4cc7381e1ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlnas.analysis.dd import distance_distribution_plot\n",
    "\n",
    "SIZE = 250\n",
    "\n",
    "rows = []\n",
    "for sm in tqdm(SUBMODULES):\n",
    "    h, e = full_ds_dd[sm][\"hist\"], full_ds_dd[sm][\"edges\"]\n",
    "    figure = distance_distribution_plot(h, e, height=SIZE, include_chi=False)\n",
    "    figure.title = (\n",
    "        \"Distance distributions: full ds (black) vs. intra-classes (green)\\n\"\n",
    "        f\"{sm}, n={DD_N_SAMPLES}, res={RESOLUTION}\"\n",
    "    )\n",
    "    for i, dat in intra_class_dd[sm].items():\n",
    "        e, h = dat[\"edges\"][:-1], dat[\"hist\"]\n",
    "        figure.line(e, h, color=\"green\", width=0.5)\n",
    "    rows.append(figure)\n",
    "\n",
    "figure = bkl.column(rows)\n",
    "bk.show(figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303b7541-b58b-40ad-8bf1-56d323739fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_png(figure, filename=LC_PATH / \"intra_class_dd.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a814784b-33b9-4b94-bd57-f128a5fe6473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bcfa6bb1-1af3-4c36-b938-e6d9cc9c6aec",
   "metadata": {},
   "source": [
    "## Inter-class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287a60b7-fd50-4ab7-af47-e86e513d75a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inter_class_dd is similar to intra_class_dd: it is a two level dict that maps\n",
    "# submodule → pair of classes (i, j) -> inter-class distance distribution\n",
    "# where the class distance distribution data is compliled in a dict with three keys:\n",
    "# - `d`: the flattened cdist distance matrix\n",
    "# - `hist` (RESOLUTION,): histogram counts\n",
    "# - `edges` (RESOLUTION + 1,): histogram bin edges\n",
    "\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Considering every class pair would be too much\n",
    "CLASS_PAIRS = list(combinations(CLASSES[:5], 2))\n",
    "print(\"Considering\", len(CLASS_PAIRS), \"class pairs\")\n",
    "\n",
    "inter_class_dd = defaultdict(dict)\n",
    "for sm, u in tqdm(latent_embeddings.items()):\n",
    "    for i, j in tqdm(CLASS_PAIRS, leave=False):\n",
    "        g = tb.GuardedBlockHandler(\n",
    "            LC_PATH\n",
    "            / \"pdist\"\n",
    "            / \"train\"\n",
    "            / \"inter-class\"\n",
    "            / f\"{i}-{j}\"\n",
    "            / (sm + \".st\")\n",
    "        )\n",
    "        for _ in g.guard():\n",
    "            ui = u[y_true == i][:DD_N_SAMPLES]\n",
    "            uj = u[y_true == j][:DD_N_SAMPLES]\n",
    "            h, e = distance_distribution(ui, uj)\n",
    "            g.result = {\"hist\": h, \"edges\": e}\n",
    "        inter_class_dd[sm][(i, j)] = g.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c04dab-5cb1-433c-aa30-069eaaea7ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 250\n",
    "\n",
    "rows = []\n",
    "for sm in tqdm(SUBMODULES):\n",
    "    h, e = full_ds_dd[sm][\"hist\"], full_ds_dd[sm][\"edges\"]\n",
    "    figure = distance_distribution_plot(h, e, height=SIZE, include_chi=False)\n",
    "    figure.title = (\n",
    "        \"Distance distribution: full ds (black) \"\n",
    "        \"    vs. intra-class (green)\\n\"\n",
    "        \"    vs. inter-classes (red)\\n\"\n",
    "        f\"{sm}, n={DD_N_SAMPLES}, res={RESOLUTION}\"\n",
    "    )\n",
    "    for i, dat in intra_class_dd[sm].items():\n",
    "        e, h = dat[\"edges\"][:-1], dat[\"hist\"]\n",
    "        figure.line(e, h, color=\"green\", width=0.5)\n",
    "    for (i, j), dat in inter_class_dd[sm].items():\n",
    "        e, h = dat[\"edges\"][:-1], dat[\"hist\"]\n",
    "        figure.line(e, h, color=\"red\", width=0.5)\n",
    "    rows.append(figure)\n",
    "\n",
    "figure = bkl.column(rows)\n",
    "bk.show(figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7bfdc1-7664-4ba6-8edd-5710217942bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_png(figure, filename=LC_PATH / \"inter_class_dd.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a085c55-d163-44e4-a5ff-914409b9e1cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67eec51c-cff6-43f4-b57a-a38e56dd802d",
   "metadata": {},
   "source": [
    "# Dim-redux\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02571821-786e-48cb-90a6-e414853f8040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider DR_N_SAMPLES for dimensionality reduction\n",
    "DR_N_SAMPLES = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf20b3f-d1c3-4f90-9691-6c35f914f28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml import UMAP\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "latent_embeddings_2d = {}\n",
    "for sm, u in tqdm(latent_embeddings.items()):\n",
    "    g = tb.GuardedBlockHandler(LC_PATH / \"umap\" / \"train\" / (sm + \".st\"))\n",
    "    for _ in g.guard():\n",
    "        e = UMAP(n_components=2).fit_transform(u[:DR_N_SAMPLES])\n",
    "        e = MinMaxScaler().fit_transform(e)\n",
    "        g.result = {\"\": e}\n",
    "    latent_embeddings_2d[sm] = g.result[\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173ab048-33ea-4234-9fe1-ef868cc662d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ab7fff1-f2ff-41a4-9575-5ab978296a77",
   "metadata": {},
   "source": [
    "# Highly confused classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a14a53-4160-4095-8fd5-b0c2b72cd24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confused_true_pred_dd is a dict that maps\n",
    "# submodule → (i_true, j_pred) -> inter-class distance distribution data\n",
    "# between true class i_true and predicted class j_pred.\n",
    "# As usual, distance distribution data are compiled into a dict\n",
    "# - `d`: the cdist distance vector\n",
    "# - `hist` (RESOLUTION,): histogram counts\n",
    "# - `edges` (RESOLUTION + 1,): histogram bin edges\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "N_CONFUSIONS = 10\n",
    "\n",
    "confused_true_pred_dd = defaultdict(dict)\n",
    "for sm, u in tqdm(latent_embeddings.items()):\n",
    "    for i_true, j_pred in tqdm(confusion_pairs[:N_CONFUSIONS], leave=False):\n",
    "        ui = u[y_true == i_true]  # [:DD_N_SAMPLES]\n",
    "        uj = u[y_pred == j_pred]  # [:DD_N_SAMPLES]\n",
    "        h, e = distance_distribution(ui, uj)\n",
    "        confused_true_pred_dd[sm][(i_true, j_pred)] = {\n",
    "            \"edges\": e,\n",
    "            \"hist\": h,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbb156b-eb21-44e1-b5c4-c196b3aa400d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.palettes import viridis\n",
    "\n",
    "SIZE = 300\n",
    "\n",
    "rows = []\n",
    "for sm, data in tqdm(confused_true_pred_dd.items()):\n",
    "    h, e = full_ds_dd[sm][\"hist\"], full_ds_dd[sm][\"edges\"]\n",
    "    figure = distance_distribution_plot(h, e, height=SIZE, include_chi=False)\n",
    "    figure.title = (\n",
    "        \"Distance distribution: full ds (black)\\n\"\n",
    "        \"vs. inter-class in high-confusion true-pred pairs (viridis, darker = more confused)\\n\"\n",
    "        f\"{sm}, n={DD_N_SAMPLES}, res={RESOLUTION}\"\n",
    "    )\n",
    "\n",
    "    everything = list(\n",
    "        zip(\n",
    "            data.items(),\n",
    "            viridis(len(data)),  # more confused = darker tones\n",
    "        )\n",
    "    )\n",
    "    everything = everything[::-1]  # draw from least to most confused\n",
    "    for ((i, j), dd), color in everything:\n",
    "        h, e = dd[\"hist\"], dd[\"edges\"][:-1]\n",
    "        figure.line(e, h, color=color, width=1)\n",
    "\n",
    "    rows.append(figure)\n",
    "\n",
    "figure = bkl.column(rows)\n",
    "bk.show(figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e85749-8ae3-43c2-a885-86113d042bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_png(figure, filename=LC_PATH / \"inter_class_confused_dd.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6010819c-06b5-4eb8-9747-819d6dd7e8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 500\n",
    "\n",
    "i_true, j_pred = confusion_pairs[0]\n",
    "p1 = (y_true == i_true) & (y_pred == i_true)  # correctly classified as i_true\n",
    "p2 = (y_true == j_pred) & (y_pred == j_pred)  # correctly classified as j_pred\n",
    "p3 = (y_true == i_true) & (y_pred == j_pred)  # i_true but classified in j_pred\n",
    "p = p1 | p2 | p3\n",
    "\n",
    "rows = []\n",
    "for sm, u in tqdm(latent_embeddings_2d.items()):\n",
    "    figure = bk.figure(width=SIZE, height=SIZE)\n",
    "    figure.title = (\n",
    "        f\"Correctly classified {i_true} → {i_true} (blue), \"\n",
    "        f\"{j_pred} → {j_pred} (green)\\n\"\n",
    "        f\"Misclassified {i_true} → {j_pred} (red)\\n\" + sm\n",
    "    )\n",
    "\n",
    "    figure.scatter(u[p1][:, 0], u[p1][:, 1], marker=\"x\", color=\"blue\", size=2)\n",
    "    figure.scatter(u[p2][:, 0], u[p2][:, 1], marker=\"x\", color=\"green\", size=1)\n",
    "    figure.scatter(u[p3][:, 0], u[p3][:, 1], color=\"red\", size=3)\n",
    "\n",
    "    rows.append(figure)\n",
    "\n",
    "figure = bkl.column(rows)\n",
    "bk.show(figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2487845-263b-42d9-8d40-f079ef3ccc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "SIZE = 500\n",
    "\n",
    "rows = []\n",
    "for sm, u in tqdm(latent_embeddings.items()):\n",
    "    u_2d = latent_embeddings_2d[sm]\n",
    "\n",
    "    index = NearestNeighbors(n_neighbors=5)\n",
    "    index.fit(u[p1])  # KNN index of all samples i_true → i_true\n",
    "    knn_dst, knn_idx = index.kneighbors(\n",
    "        u[p3]\n",
    "    )  # KNNs of all samples i_true → j_pred\n",
    "\n",
    "    figure = bk.figure(width=SIZE, height=SIZE)\n",
    "    figure.title = (\n",
    "        f\"Correctly classified {i_true} → {i_true} (blue), {j_pred} → {j_pred} (green)\\n\"\n",
    "        f\"Misclassified {i_true} → {j_pred} (red)\\n\" + sm\n",
    "    )\n",
    "\n",
    "    figure.scatter(u_2d[p1][:, 0], u_2d[p1][:, 1], color=\"blue\", size=2)\n",
    "    figure.scatter(u_2d[p2][:, 0], u_2d[p2][:, 1], color=\"green\", size=1)\n",
    "    figure.scatter(u_2d[p3][:, 0], u_2d[p3][:, 1], color=\"red\", size=4)\n",
    "\n",
    "    # Iterate over all 2D repr. of misclassified samples i_true → j_pred\n",
    "    for ia, a in enumerate(u_2d[p3]):\n",
    "        ib, d = knn_idx[ia, 0], knn_dst[ia, 0]\n",
    "        b, width = u_2d[p1][ib], 1 / (1 + np.exp(-d)) * 2 - 1\n",
    "        figure.line([a[0], b[0]], [a[1], b[1]], color=\"black\", width=width)\n",
    "\n",
    "    rows.append(figure)\n",
    "\n",
    "figure = bkl.column(rows)\n",
    "bk.show(figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1b4792-2c3b-49a3-94a4-9d4eb2904feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_png(figure, filename=LC_PATH / \"knn_error_correction.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b147e8-519e-45d8-a7dd-ef8cf0ee5fdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c7c768f-53b4-4f19-a8b7-b435ab5e679e",
   "metadata": {},
   "source": [
    "# LCC on highly confused classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3254a9ac-578b-44b4-b5fe-2e4e86eb9acf",
   "metadata": {},
   "source": [
    "In this section we consider the subset of sample belonging to the most highly confused classes, and the logits associated with them. We can think of these logits as latent embeddings in a n_classes-dimensional latent space. We are interested in applying latent cluster correction in this setting.\n",
    "\n",
    "Of course this is a simplified setting since we operate directly on the latent representations rather than the weights that produced them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dfd361-36dd-45a7-bce8-7bb4c13f731f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import hamming_loss\n",
    "from torch import optim\n",
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "from nlnas.correction import clustering_loss, otm_matching_predicates\n",
    "\n",
    "\n",
    "def standardize_labels(\n",
    "    y: torch.Tensor, matching: dict[int, set[int]]\n",
    ") -> tuple[np.ndarray, dict[int, set[int]]]:\n",
    "    \"\"\"\n",
    "    The keys of `matching` must be among the values of `y`.\n",
    "\n",
    "    Returns an equivalent label vector and matching but where the values of the\n",
    "    label vector are consecutive numbers starting from 0.\n",
    "    \"\"\"\n",
    "    _y, _m = torch.zeros_like(y), {}\n",
    "    for i_new, i_old in enumerate(torch.unique(y)):\n",
    "        _y[y == i_old], _m[i_new] = i_new, matching[int(i_old)]\n",
    "    return _y, _m\n",
    "\n",
    "\n",
    "def full_louvain_pipeline(\n",
    "    u: torch.Tensor,\n",
    "    yt: torch.Tensor,\n",
    "    k: int = 10,\n",
    ") -> tuple[\n",
    "    np.ndarray, list[set[int]], dict[int, set[int]], torch.Tensor, np.ndarray\n",
    "]:\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    * the Louvain cluster label vector;\n",
    "    * the Louvain communities;\n",
    "    * a matching between the true labels of `yt` and the Louvain labels;\n",
    "    * the Louvain loss\n",
    "    * the two-dimentional missclustering predicate: words, `p3[a, i]` is `True`\n",
    "      if sample `i` is in true class `a` but not in any  Louvain class matched\n",
    "      with `a`.\n",
    "    \"\"\"\n",
    "    if not isinstance(u, torch.Tensor):\n",
    "        u = torch.tensor(u)\n",
    "    communities, yc = louvain_communities(u, k=k)\n",
    "    matching = class_otm_matching(yt, yc)\n",
    "    _yt, _m = standardize_labels(yt, matching)\n",
    "    _, _, p_miss, _ = otm_matching_predicates(_yt, yc, _m)\n",
    "    loss = clustering_loss(u, _yt, yc, _m, k)\n",
    "    return yc, communities, matching, loss, p_miss\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    u: torch.Tensor,\n",
    "    y_true: torch.Tensor,\n",
    "    weight_clst: float = 1,\n",
    "    weight_ce: float = 1,\n",
    "    k: int = 20,\n",
    "    extra_data: bool = True,\n",
    ") -> tuple[torch.Tensor, dict]:\n",
    "    \"\"\"Returns the loss and a bunch of data in a dict\"\"\"\n",
    "    y_clst, communities, matching, loss_clst, p_miss = full_louvain_pipeline(\n",
    "        u, y_true, k=k\n",
    "    )\n",
    "    loss_ce = cross_entropy(u, y_true)\n",
    "    loss = weight_ce * loss_ce + weight_clst * loss_clst\n",
    "    u_detach = u.detach().cpu().numpy().copy()\n",
    "    y_pred = u_detach.argmax(axis=-1)\n",
    "    epoch_data = {\n",
    "        \"loss_clst\": loss_clst.item(),\n",
    "        \"loss_ce\": loss_ce.item(),\n",
    "        \"loss\": loss.item(),\n",
    "        \"acc\": accuracy_score(y_true.detach().cpu(), y_pred),\n",
    "        \"n_err\": (y_true.cpu().numpy() != y_pred).sum(),\n",
    "        \"n_miss\": p_miss.sum(),\n",
    "        \"n_communities\": len(communities),\n",
    "        \"n_unmatched\": sum(len(v) == 0 for v in matching.values()),\n",
    "    }\n",
    "    if extra_data:\n",
    "        epoch_data[\"u\"] = u_detach\n",
    "        epoch_data[\"communities\"] = communities\n",
    "        epoch_data[\"matching\"] = matching\n",
    "        epoch_data[\"y_clst\"] = y_clst\n",
    "        epoch_data[\"y_pred\"] = y_pred\n",
    "        epoch_data[\"p_miss\"] = p_miss\n",
    "    return loss, epoch_data\n",
    "\n",
    "\n",
    "def louvain_descent(\n",
    "    u0: np.ndarray,\n",
    "    y_true: np.ndarray | torch.Tensor,\n",
    "    n_epochs: int,\n",
    "    weight_clst: float = 1,\n",
    "    weight_ce: float = 1,\n",
    "    k: int = 20,\n",
    "    lr: float = 1e-3,\n",
    "    extra_data: bool = True,\n",
    ") -> list[dict]:\n",
    "    DEVICE = \"cuda:0\"\n",
    "    u = torch.tensor(u0, requires_grad=True, device=DEVICE)\n",
    "    if isinstance(y_true, np.ndarray):\n",
    "        y_true = torch.tensor(y_true).long()\n",
    "    y_true = y_true.to(DEVICE)\n",
    "    optimizer = optim.Adam([u], lr=lr)\n",
    "    training_data = []\n",
    "\n",
    "    _eval = partial(\n",
    "        evaluate,\n",
    "        y_true=y_true,\n",
    "        weight_clst=weight_clst,\n",
    "        weight_ce=weight_ce,\n",
    "        k=k,\n",
    "        extra_data=extra_data,\n",
    "    )\n",
    "\n",
    "    # BEFORE TRAINING\n",
    "    loss, initial_data = _eval(u)\n",
    "    training_data.append(initial_data)\n",
    "\n",
    "    # TRAINING\n",
    "    progress = tqdm(range(n_epochs))\n",
    "    for i in progress:\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        loss, epoch_data = _eval(u)\n",
    "        training_data.append(epoch_data)\n",
    "        progress.set_postfix(\n",
    "            {\n",
    "                \"loss_clst\": np.round(epoch_data[\"loss_clst\"], 3),\n",
    "                \"acc\": np.round(epoch_data[\"acc\"], 3),\n",
    "                \"n_err\": epoch_data[\"n_err\"],\n",
    "                \"n_miss\": epoch_data[\"n_miss\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a487f2-0ddc-4cb5-ab58-5f6ed1370341",
   "metadata": {},
   "source": [
    "We create a predicate (aka a mask aka a boolean array) to select samples from the top `N_CONFUSION_PAIRS` pairs of most confused classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3d37a8-7075-4029-af69-adfaa8565f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlnas.correction import class_otm_matching, louvain_communities\n",
    "\n",
    "N_CONFUSION_PAIRS = 10\n",
    "\n",
    "p = np.full_like(y_true, False, dtype=bool)\n",
    "for i_true, j_pred in confusion_pairs[:N_CONFUSION_PAIRS]:\n",
    "    p1 = (y_true == i_true) & (y_pred == i_true)  # i_true -> i_true\n",
    "    p2 = (y_true == j_pred) & (y_pred == j_pred)  # j_pred -> j_pred\n",
    "    p3 = (y_true == i_true) & (y_pred == j_pred)  # i_true -> j_pred\n",
    "    print(\n",
    "        \"Label pair\",\n",
    "        i_true,\n",
    "        \"&\",\n",
    "        j_pred,\n",
    "        \":\",\n",
    "        p3.sum(),\n",
    "        \"missclassified samples\",\n",
    "    )\n",
    "    p = p | p1 | p2 | p3\n",
    "print(\"Total:              \", p.sum(), \"samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a74d8c1-fba6-4cc0-892e-56a3643e3e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 1000\n",
    "\n",
    "K = 2\n",
    "WEIGHT_CLST, WEIGHT_CE = 1, 1e-5\n",
    "LR = 1e-3\n",
    "\n",
    "training_data = louvain_descent(\n",
    "    latent_embeddings[HEAD_NAME][p],\n",
    "    y_true[p],\n",
    "    n_epochs=N_EPOCHS,\n",
    "    weight_clst=WEIGHT_CLST,\n",
    "    weight_ce=WEIGHT_CE,\n",
    "    k=K,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cd0465-0e9f-4f74-b5fd-e897c0f9143a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "kw = {\"width\": 500, \"height\": 200, \"toolbar_location\": None}\n",
    "x = np.arange(len(training_data))\n",
    "\n",
    "figure = bk.figure(title=\"Louvain loss\", **kw)\n",
    "figure.line(x, [d[\"loss_clst\"] for d in training_data])\n",
    "rows.append(figure)\n",
    "\n",
    "figure = bk.figure(title=\"Accuracy\", **kw)\n",
    "figure.line(x, [d[\"acc\"] for d in training_data])\n",
    "rows.append(figure)\n",
    "\n",
    "figure = bk.figure(title=\"Nb. of errors\", **kw)\n",
    "figure.line(x, [d[\"n_err\"] for d in training_data])\n",
    "rows.append(figure)\n",
    "\n",
    "figure = bk.figure(title=\"Nb. of clusters\", **kw)\n",
    "figure.line(x, [d[\"n_communities\"] for d in training_data])\n",
    "rows.append(figure)\n",
    "\n",
    "figure = bk.figure(title=\"Nb. missclustered samples\", **kw)\n",
    "figure.line(x, [d[\"n_miss\"] for d in training_data])\n",
    "rows.append(figure)\n",
    "\n",
    "param_str = (\n",
    "    f\"w_clst={WEIGHT_CLST}, w_ce={WEIGHT_CE}, k={K}, lr={LR}, n={p.sum()}\"\n",
    ")\n",
    "for figure in rows:\n",
    "    figure.title.text += \"\\n\" + param_str\n",
    "\n",
    "figure = bkl.column(rows)\n",
    "bk.show(figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d61c1af-dcbb-461c-870c-6a55e5c3ea3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_png(\n",
    "    figure,\n",
    "    filename=LC_PATH\n",
    "    / f\"lcc_metrics_top_confused_wclst={WEIGHT_CLST}_wce={WEIGHT_CE}_k={K}_lr={LR}.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e5b927-36d3-47ee-b865-7bfa608c75c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml import UMAP\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from nlnas.plotting import class_scatter\n",
    "\n",
    "N_SNAPSHOTS = 10\n",
    "SIZE = 400\n",
    "\n",
    "rows = []\n",
    "kw = {\"width\": SIZE, \"height\": SIZE, \"toolbar_location\": None}\n",
    "\n",
    "for i in tqdm(np.linspace(0, len(training_data) - 1, N_SNAPSHOTS, dtype=int)):\n",
    "    rows.append([])\n",
    "\n",
    "    d = training_data[i]\n",
    "    u_2d = UMAP().fit_transform(d[\"u\"])\n",
    "    u_2d = MinMaxScaler().fit_transform(u_2d)\n",
    "    u_err_2d = u_2d[d[\"y_pred\"] != y_true[p]]\n",
    "    n_err = d[\"n_err\"]\n",
    "    is_miss = d[\"p_miss\"].sum(axis=0) > 0\n",
    "    u_miss_2d, n_miss = u_2d[is_miss], is_miss.sum()\n",
    "\n",
    "    figure = bk.figure(\n",
    "        title=f\"[epoch={i}] True labels, missclassified in red (n={n_err})\",\n",
    "        **kw,\n",
    "    )\n",
    "    class_scatter(figure, u_2d, y_true[p])\n",
    "    figure.scatter(u_err_2d[:, 0], u_err_2d[:, 1], size=3, color=\"red\")\n",
    "    rows[-1].append(figure)\n",
    "\n",
    "    figure = bk.figure(title=f\"[epoch={i}] Louvain labels\", **kw)\n",
    "    class_scatter(figure, u_2d, d[\"y_clst\"])\n",
    "    rows[-1].append(figure)\n",
    "\n",
    "    figure = bk.figure(\n",
    "        title=f\"[epoch={i}] Louvain labels, misclustered in red (n={n_miss})\",\n",
    "        **kw,\n",
    "    )\n",
    "    class_scatter(figure, u_2d, d[\"y_clst\"])\n",
    "    figure.scatter(u_miss_2d[:, 0], u_miss_2d[:, 1], size=3, color=\"red\")\n",
    "    rows[-1].append(figure)\n",
    "\n",
    "param_str = (\n",
    "    f\"w_clst={WEIGHT_CLST}, w_ce={WEIGHT_CE}, k={K}, lr={LR}, n={p.sum()}\"\n",
    ")\n",
    "for r in rows:\n",
    "    for figure in r:\n",
    "        figure.title.text += \"\\n\" + param_str\n",
    "\n",
    "figure = bkl.grid(rows)\n",
    "bk.show(figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12132417-4b67-4254-b888-862f341997be",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_png(\n",
    "    figure,\n",
    "    filename=LC_PATH\n",
    "    / f\"lcc_umap_top_confused_wclst={WEIGHT_CLST}_wce={WEIGHT_CE}_k={K}_lr={LR}.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ebdc1f-790d-44bd-88c4-baff82da4928",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SNAPSHOTS = 10\n",
    "SIZE = 250\n",
    "\n",
    "rows = []\n",
    "kw = {\"width\": 2 * SIZE, \"height\": SIZE, \"toolbar_location\": None}\n",
    "\n",
    "for i in tqdm(np.linspace(0, len(training_data) - 1, N_SNAPSHOTS, dtype=int)):\n",
    "    epoch_data = training_data[i]\n",
    "    v, yt = epoch_data[\"u\"], y_true[p]\n",
    "\n",
    "    figure = bk.figure(**kw)\n",
    "    figure.title = (\n",
    "        f\"[epoch={i}] Full dataset DD (black)\\n\"\n",
    "        \"vs. inter-class DD btw highly-confused class pairs (green)\\n\"\n",
    "        \"vs. intra-class DD (red)\"\n",
    "    )\n",
    "\n",
    "    h, e = distance_distribution(v)\n",
    "    figure.line(e[:-1], h, width=2, color=\"black\")\n",
    "\n",
    "    for i_true, j_pred in tqdm(\n",
    "        confusion_pairs[:N_CONFUSION_PAIRS], leave=False\n",
    "    ):\n",
    "        h, e = distance_distribution(v[yt == i_true], v[yt == j_pred])\n",
    "        figure.line(e[:-1], h, color=\"green\")\n",
    "        h, e = distance_distribution(v[yt == i_true])\n",
    "        figure.line(e[:-1], h, color=\"red\", width=0.25)\n",
    "        h, e = distance_distribution(v[yt == j_pred])\n",
    "        figure.line(e[:-1], h, color=\"red\", width=0.25)\n",
    "\n",
    "    rows.append(figure)\n",
    "\n",
    "param_str = (\n",
    "    f\"w_clst={WEIGHT_CLST}, w_ce={WEIGHT_CE}, k={K}, lr={LR}, n={p.sum()}\"\n",
    ")\n",
    "for figure in rows:\n",
    "    figure.title.text += \"\\n\" + param_str\n",
    "\n",
    "figure = bkl.column(rows)\n",
    "bk.show(figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6dfa18-32d7-48d0-b99b-d6df65fd554d",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_png(\n",
    "    figure,\n",
    "    filename=LC_PATH\n",
    "    / f\"lcc_dd_top_confused_wclst={WEIGHT_CLST}_wce={WEIGHT_CE}_k={K}_lr={LR}.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bfc617-6087-4e70-8adc-0ed4ba312fbf",
   "metadata": {},
   "source": [
    "# LCC on whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8f4564-d8c4-4e1c-b01e-65a036825f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_embeddings[HEAD_NAME].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e253148-9f08-4dd8-9f31-decd17cf8132",
   "metadata": {},
   "outputs": [],
   "source": [
    "LCC_N_SAMPLES = len(latent_embeddings[HEAD_NAME]) // 1\n",
    "LCC_N_SAMPLES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78682dd0-5e10-4072-884a-1f03ef0991e4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Finding the right $k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8832ae-386e-4e7c-82f6-5de682da5a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlnas.correction import otm_matching_predicates\n",
    "\n",
    "K = 50\n",
    "\n",
    "y_clst, communities, matching, loss, p_miss = full_louvain_pipeline(\n",
    "    latent_embeddings[HEAD_NAME][:LCC_N_SAMPLES], y_true[:LCC_N_SAMPLES], k=K\n",
    ")\n",
    "print(\"Found\", len(communities), \"communities\")\n",
    "print(\"Loss:\", np.round(loss.item(), 5))\n",
    "\n",
    "n_classes = len(np.unique(y_true[:LCC_N_SAMPLES]))\n",
    "n_unmatched = sum(len(v) == 0 for v in matching.values())\n",
    "n_miss = p_miss.sum()\n",
    "print(\n",
    "    \"Unmatched true classes:\",\n",
    "    n_unmatched,\n",
    "    \"->\",\n",
    "    np.round(n_unmatched / n_classes * 100, 3),\n",
    "    \"%\",\n",
    ")\n",
    "print(\n",
    "    \"Missclustered samples:\",\n",
    "    n_miss,\n",
    "    \"->\",\n",
    "    np.round(n_miss / LCC_N_SAMPLES * 100, 3),\n",
    "    \"%\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010a2adc-7f1b-4598-ab17-67d0301a77b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"k\",\n",
    "        \"n_communities\",\n",
    "        \"loss_clst\",\n",
    "        \"n_unmatched\",\n",
    "        \"n_miss\",\n",
    "        \"r_unmatched\",\n",
    "        \"r_miss\",\n",
    "        \"community_size\",\n",
    "    ]\n",
    ")\n",
    "ks = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "n_samples, n_classes = len(y_true), len(np.unique(y_true))\n",
    "\n",
    "for k in tqdm(ks):\n",
    "    y_clst, communities, matching, loss, p_miss = full_louvain_pipeline(\n",
    "        latent_embeddings[HEAD_NAME][:LCC_N_SAMPLES],\n",
    "        y_true[:LCC_N_SAMPLES],\n",
    "        k=k,\n",
    "    )\n",
    "    n_unmatched = sum(len(v) == 0 for v in matching.values())\n",
    "    n_miss = p_miss.sum()\n",
    "    df.loc[len(df)] = {\n",
    "        \"k\": k,\n",
    "        \"n_communities\": len(communities),\n",
    "        \"community_size\": [len(c) for c in communities],\n",
    "        \"loss_clst\": loss.item(),\n",
    "        \"n_unmatched\": n_unmatched,\n",
    "        \"r_unmatched\": n_unmatched / n_classes,\n",
    "        \"n_miss\": n_miss,\n",
    "        \"r_miss\": n_miss / n_samples,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b742a8-fe3c-4de6-9ad9-5f019ea4f277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10, 5))\n",
    "sns.lineplot(data=df, x=\"k\", y=\"loss_clst\", ax=ax[0][0])\n",
    "sns.lineplot(data=df, x=\"k\", y=\"n_communities\", ax=ax[0][1])\n",
    "sns.lineplot(data=df, x=\"k\", y=\"r_unmatched\", ax=ax[1][0])\n",
    "sns.lineplot(data=df, x=\"k\", y=\"r_miss\", ax=ax[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cf4eaa-c7ea-4e16-830e-b330268e3c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = df[[\"k\", \"community_size\"]].explode(\"community_size\")\n",
    "sns.boxplot(data=_df, x=\"k\", y=\"community_size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da431daa-81c1-4197-a0ff-412b71aee0f9",
   "metadata": {},
   "source": [
    "## LCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db393da5-d0e4-4dac-85e4-a6e75f584af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 100\n",
    "\n",
    "K = 2\n",
    "WEIGHT_CLST, WEIGHT_CE = 1, 0\n",
    "LR = 1e-3\n",
    "\n",
    "training_data = louvain_descent(\n",
    "    latent_embeddings[HEAD_NAME][:LCC_N_SAMPLES],\n",
    "    y_true[:LCC_N_SAMPLES],\n",
    "    n_epochs=N_EPOCHS,\n",
    "    weight_clst=WEIGHT_CLST,\n",
    "    weight_ce=WEIGHT_CE,\n",
    "    k=K,\n",
    "    extra_data=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f59883c-ea1e-4ded-982a-81a2eb198f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "kw = {\"width\": 500, \"height\": 200, \"toolbar_location\": None}\n",
    "x = np.arange(len(training_data))\n",
    "\n",
    "figure = bk.figure(title=\"Louvain loss\", **kw)\n",
    "figure.line(x, [d[\"loss_clst\"] for d in training_data])\n",
    "rows.append(figure)\n",
    "\n",
    "figure = bk.figure(title=\"Accuracy\", **kw)\n",
    "figure.line(x, [d[\"acc\"] for d in training_data])\n",
    "rows.append(figure)\n",
    "\n",
    "figure = bk.figure(title=\"Nb. of errors\", **kw)\n",
    "figure.line(x, [d[\"n_err\"] for d in training_data])\n",
    "rows.append(figure)\n",
    "\n",
    "figure = bk.figure(title=\"Nb. of clusters\", **kw)\n",
    "figure.line(x, [d[\"n_communities\"] for d in training_data])\n",
    "rows.append(figure)\n",
    "\n",
    "figure = bk.figure(title=\"Nb. missclustered samples\", **kw)\n",
    "figure.line(x, [d[\"n_miss\"] for d in training_data])\n",
    "rows.append(figure)\n",
    "\n",
    "param_str = f\"w_clst={WEIGHT_CLST}, w_ce={WEIGHT_CE}, k={K}, lr={LR}, n={LCC_N_SAMPLES}\"\n",
    "for figure in rows:\n",
    "    figure.title.text += \"\\n\" + param_str\n",
    "\n",
    "figure = bkl.column(rows)\n",
    "bk.show(figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2aa3ab-780d-401d-b0b7-375a3652abc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_png(\n",
    "    figure,\n",
    "    filename=LC_PATH / f\"lcc_metrics_full_ds_w={WEIGHT}_k={K}_lr={LR}.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ef7427-ab1b-4523-9a3c-5e1a0948c7e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26b5381-5ec9-41e5-89f2-3535df2eb672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c72326-26ec-476c-8df3-1fc8b1d88bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e13421f-72fe-4601-9118-add22d3a8d36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd76cd0b-924c-49b2-a22f-5052980500dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlnas",
   "language": "python",
   "name": "nlnas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
