{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59add90d-20a6-400b-885b-8068b110dc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import pandas as pd\n",
    "import turbo_broccoli as tb\n",
    "import seaborn as sns\n",
    "import bokeh.plotting as bk\n",
    "import bokeh.layouts as bkl\n",
    "from bokeh.io import output_notebook, export_png\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nlnas import *\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff207491-6753-4399-a10d-e931bd1e1973",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Evaluation UMAPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1677fa0-b180-4144-82e0-3974d1e0faec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"resnet18\"\n",
    "dataset_name = \"cifar10\"\n",
    "version = 0\n",
    "epoch = 28\n",
    "\n",
    "path = Path(\"out\") / model_name / dataset_name / f\"version_{version}\" / str(epoch)\n",
    "\n",
    "data_plts = tb.load(path / \"umap\" / \"plots.json\")\n",
    "figures = list(data_plts.values())\n",
    "for fig in figures:\n",
    "    fig.title.text = \"\"\n",
    "    fig.toolbar_location = None\n",
    "    fig.xgrid.visible = False\n",
    "    fig.ygrid.visible = False\n",
    "    fig.axis.visible = False\n",
    "    fig.height = 250\n",
    "    fig.width = 250\n",
    "figure = bkl.row(figures)\n",
    "export_png(figure, filename=\"out/resnet18_cifar10.png\")\n",
    "bk.show(figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cc804d-58a2-4634-a676-8b540e776c76",
   "metadata": {},
   "source": [
    "# More than one cluster per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb5f777-b2e6-4cf1-8159-9aab07a90da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"alexnet\"\n",
    "dataset_name = \"fashionmnist\"\n",
    "version = 0\n",
    "epoch = 25\n",
    "layer = \"model.classifier.4\"\n",
    "\n",
    "path = Path(\"out\") / model_name / dataset_name / f\"version_{version}\" / str(epoch)\n",
    "data = tb.load(path / \"umap\" / \"plots.json\")\n",
    "fig_all = data[layer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9841b615-ae5d-43f5-87f3-a9393068eb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eval = tb.load(path / \"eval\" / \"eval.json\")\n",
    "z, y_true = data_eval[\"z\"][layer].flatten(1).numpy(), data_eval[\"y_true\"].numpy()\n",
    "\n",
    "data_umap = tb.load(path / \"umap\" / \"umap.st\")\n",
    "e = data_umap[layer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31298494-519b-41a3-9a4d-dcf738f960ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh import palettes as bkp\n",
    "from nlnas.plotting import class_scatter\n",
    "\n",
    "size, cls = 250, 9\n",
    "color = bkp.viridis(10)[cls]\n",
    "\n",
    "fig_1 = bk.figure()\n",
    "class_scatter(fig_1, e, y_true == cls, palette=[\"#ffffff\", color])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ad0d07-638f-4082-aff1-38e70b607e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh import layouts as bkl\n",
    "\n",
    "for f in [fig_all, fig_1]:\n",
    "    f.title.text = \"\"\n",
    "    f.toolbar_location = None\n",
    "    f.xgrid.visible = False\n",
    "    f.ygrid.visible = False\n",
    "    f.axis.visible = False\n",
    "    f.height = 250\n",
    "    f.width = 250\n",
    "figure = bkl.row([fig_all, fig_1])\n",
    "export_png(figure, filename=\"out/alexnet_fashionmnist_umap.png\")\n",
    "bk.show(figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b46536-77c3-49f4-9489-99efbe84411e",
   "metadata": {},
   "source": [
    "## Trying to manually find samples from each clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fbf502-2c99-4a98-9bd7-912210310734",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_a = ((e[:,1] > .7) & (y_true == cls)).nonzero()[0][0]\n",
    "idx_b = ((e[:,1] < .2) & (y_true == cls)).nonzero()[0][9]\n",
    "\n",
    "a = data_eval[\"x\"][idx_a]\n",
    "b = data_eval[\"x\"][idx_b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bda02d-7cbe-4090-9f70-202b1b52b5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "from nlnas.tv_dataset import TorchvisionDataset\n",
    "from nlnas.transforms import EnsureRGB, dataset_normalization\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        EnsureRGB(),\n",
    "        dataset_normalization(\"fashionmnist\"),\n",
    "        transforms.Resize([64, 64], antialias=True),\n",
    "    ]\n",
    ")\n",
    "ds = FashionMNIST(\n",
    "    root=\"/home/cedric/torchvision/datasets/\",\n",
    "    transform=transform,\n",
    "    train=True,\n",
    ")\n",
    "\n",
    "i_a, i_b = -1, -1\n",
    "for i, (x, y) in enumerate(tqdm(ds)):\n",
    "    if i_a >= 0 and i_b >= 0:\n",
    "        break\n",
    "    if i_a < 0 and torch.isclose(x, a, atol=1).all():\n",
    "        i_a = i\n",
    "        print(\"i_a =\", i_a)\n",
    "    if i_b < 0 and torch.isclose(x, b, atol=1).all():\n",
    "        i_b = i\n",
    "        print(\"i_b =\", i_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbb625b-6f1e-4a81-93c9-772d7d71769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        # transforms.ToTensor(),\n",
    "        # EnsureRGB(),\n",
    "        transforms.Resize([128, 128], antialias=True),\n",
    "    ]\n",
    ")\n",
    "ds = FashionMNIST(\n",
    "    root=\"/home/cedric/torchvision/datasets/\",\n",
    "    transform=transform,\n",
    "    train=True,\n",
    ")\n",
    "\n",
    "img_a, img_b = ds[i_a][0], ds[i_b][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d45d100-cfec-4c72-a174-b5494f4bb423",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d1480a-f03b-4166-8058-3d553147fcb1",
   "metadata": {},
   "source": [
    "## Multiple clusters per class: super pathological edition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1258dd43-093b-433a-8c76-0ba9716b3dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"alexnet\"\n",
    "dataset_name = \"pcam\"\n",
    "k, bs, we = 5, 2048, 3\n",
    "version = 0\n",
    "epoch = 17\n",
    "\n",
    "experiment = f\"{model_name}_finetune_l{k}_b{bs}_1e-{we}\"\n",
    "path = Path(\"out\") / experiment / dataset_name / f\"version_{version}\" / str(epoch)\n",
    "\n",
    "data_plts = tb.load(path / \"umap\" / \"plots.json\")\n",
    "figures = list(data_plts.values())\n",
    "for fig in figures:\n",
    "    fig.title.text = \"\"\n",
    "    fig.toolbar_location = None\n",
    "    fig.xgrid.visible = False\n",
    "    fig.ygrid.visible = False\n",
    "    fig.axis.visible = False\n",
    "    fig.height = 250\n",
    "    fig.width = 250\n",
    "    \n",
    "figure = bkl.grid([figures[:-3], figures[-3:] + ([None] * 2)])\n",
    "export_png(figure, filename=f\"out/{experiment}_umap.png\")\n",
    "bk.show(figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e6529f-b911-4f3e-b7a9-8d2fdbaba3c8",
   "metadata": {},
   "source": [
    "# UMAP of a latent space throughout training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49823073-3cc4-4b69-aeb0-fc8715ccfdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlnas.training import best_epoch\n",
    "\n",
    "model_name = \"googlenet\"\n",
    "dataset_name = \"cifar10\"\n",
    "version = 0\n",
    "\n",
    "be = best_epoch(\n",
    "    Path(\"out\") / model_name / dataset_name / \"model\" / \"csv_logs\" / model_name / f\"version_{version}\" / \"metrics.csv\"\n",
    ")\n",
    "print(\"Best epoch:\", be)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c1ba74-727c-4842-9441-740b528b18b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 5\n",
    "layer = \"model.inception5b\"\n",
    "\n",
    "epochs = np.linspace(0, be, num=n_epoch, dtype=int)\n",
    "print(\"Epochs:\", epochs)\n",
    "\n",
    "path = Path(\"out\") / model_name / dataset_name / f\"version_{version}\"\n",
    "all_e = [\n",
    "    tb.load(path / str(e) / \"umap\" / \"umap.st\")[layer] for e in epochs\n",
    "]\n",
    "all_eval_data = [\n",
    "    tb.load(path / str(e) / \"eval\" / \"eval.json\") for e in epochs\n",
    "]\n",
    "all_z = [d[\"z\"][layer] for d in all_eval_data]\n",
    "all_y_true = [d[\"y_true\"] for d in all_eval_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4973dc6-2600-48f5-b4a0-88d81b238944",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlnas.clustering import louvain_loss\n",
    "\n",
    "for z, y_true in zip(all_z, all_y_true):\n",
    "    print(louvain_loss(z, y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796170db-fc08-4fb7-b283-ed1c988449d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "figures = []\n",
    "for e, y_true in zip(all_e, all_y_true):\n",
    "    figure = bk.figure(height = 250, width = 250)\n",
    "    figure.title.text = \"\"\n",
    "    figure.toolbar_location = None\n",
    "    figure.xgrid.visible = False\n",
    "    figure.ygrid.visible = False\n",
    "    figure.axis.visible = False\n",
    "    class_scatter(figure, z, y_true)\n",
    "    figures.append(figure)\n",
    "\n",
    "figure = bkl.row(figures)\n",
    "bk.show(figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec57fff-5cd6-4f7f-a301-3863686b5596",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "104180a2-1a3c-40af-bede-b44ebd60be5e",
   "metadata": {},
   "source": [
    "# Louvain loss stagnates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f4b58b-b340-4ad1-899b-4e566e0d574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"alexnet\"\n",
    "dataset_name = \"cifar10\"\n",
    "k, bs, we = 5, 2048, 3\n",
    "version = 0\n",
    "\n",
    "experiment = f\"{model_name}_finetune_l{k}_b{bs}_1e-{we}\"\n",
    "be = best_epoch(\n",
    "    Path(\"out\") / experiment / dataset_name / \"model\" / \"csv_logs\" / experiment / f\"version_{version}\" / \"metrics.csv\"\n",
    ")\n",
    "print(\"Best epoch:\", be)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b9a682-6dd7-40b8-a53b-618aa2f8c2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"out\") / experiment / dataset_name / f\"version_{version}\"\n",
    "\n",
    "layer = \"model.classifier.4\"\n",
    "\n",
    "all_eval_data = [\n",
    "    tb.load(path / str(e) / \"eval\" / \"eval.json\") \n",
    "    for e in range(be)\n",
    "]\n",
    "all_z = [d[\"z\"][layer] for d in all_eval_data]\n",
    "all_y_true = [d[\"y_true\"].numpy() for d in all_eval_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b105dbb7-ea4b-47e6-b734-8e290a288e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlnas.clustering import *\n",
    "\n",
    "df = pd.DataFrame(columns=[\"epoch\", \"loss\", \"miss\"])\n",
    "for i, (z, y_true) in enumerate(zip(all_z, all_y_true)):\n",
    "    _, y_louvain = louvain_communities(z)\n",
    "    matching = class_otm_matching(y_true, y_louvain)\n",
    "    loss = float(clustering_loss(z, y_true, y_louvain, matching, k))\n",
    "    p1, p2, p3, p4 = otm_matching_predicates(y_true, y_louvain, matching)\n",
    "    miss = int(p3.sum()) / len(z)\n",
    "    print(\"Epoch\", i, \"/\", be, \"loss\", loss, \"miss\", miss)\n",
    "    df.loc[len(df)] = {\"epoch\": i, \"loss\": loss, \"miss\": miss}\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b7acb4-36b1-4570-b671-b65ec993b2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "fig, ax1 = plt.subplots() \n",
    "ax2 = ax1.twinx() \n",
    "\n",
    "sns.lineplot(df, x=\"epoch\", y=\"loss\", ax=ax1, color=\"red\")\n",
    "sns.lineplot(df, x=\"epoch\", y=\"miss\", ax=ax2, color=\"blue\")\n",
    "\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"$\\\\mathcal{L}_{\\\\mathrm{Louvain}}$\", color=\"red\")\n",
    "ax2.set_ylabel(\"$N_{\\\\mathrm{miss}} / N$\", color=\"blue\")\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(f\"out/{experiment}_{dataset_name}_loss_vs_miss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3977573e-15fa-4f6a-bb1b-af0bf41a04a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e4127b3-a485-4333-b981-9e611d633e67",
   "metadata": {},
   "source": [
    "# LaTeX tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f01221-880f-4d99-afe0-ebac9388d112",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import regex as re\n",
    "from loguru import logger as logging\n",
    "\n",
    "all_dfs = []\n",
    "\n",
    "for path in glob(\"out/*\"):\n",
    "    experiment_name = path.split(\"/\")[-1]\n",
    "    if m := re.match(r\"^([^_\\.]+)$\", experiment_name):\n",
    "        model, k, batch_size, weight = m.group(1), 0, 0, 0\n",
    "        # print(path, \"-> standard\", model)\n",
    "    elif m := re.match(r\"^(\\w+?)_finetune_l(\\d+)_b(\\d+)_1e-(\\d+)$\", experiment_name):\n",
    "        model, k, batch_size, weight = m.group(1), int(m.group(2)), int(m.group(3)), int(m.group(4))\n",
    "        # print(path, \"-> finetune\", model, k, batch_size, weight)\n",
    "    else:\n",
    "        print(\"Skipping\", path)\n",
    "    # weight_str = (\n",
    "    #     \"$10^{-\" + str(weight) + \"}$\"\n",
    "    #     if int(weight) > 0\n",
    "    #     else \"$1$\"\n",
    "    # )\n",
    "    for subpath in glob(path + \"/*\"):\n",
    "        dataset_name = subpath.split(\"/\")[-1]\n",
    "        metrics_path = subpath + \"/model/csv_logs/\" + experiment_name + \"/version_0/metrics.csv\"\n",
    "        try:\n",
    "            df = pd.read_csv(metrics_path)\n",
    "            df = df[~df[\"val/acc\"].isna()]\n",
    "            df = df[[\"val/acc\"]]\n",
    "            df[\"model\"] = model\n",
    "            df[\"k\"] = k\n",
    "            df[\"batch_size\"] = batch_size\n",
    "            df[\"weight\"] = 10 ** (-weight)\n",
    "            # df[\"weight_str\"] = weight_str\n",
    "            df[\"dataset\"] = dataset_name\n",
    "            all_dfs.append(df.loc[[df[\"val/acc\"].idxmax()]])\n",
    "        except Exception as e:\n",
    "            print(\"ERROR:\", metrics_path, type(e), str(e))\n",
    "            # logging.error(\"Could not load {}: {} {}\", metrics_path, e.__class__.__name__, str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fee5dc-063e-4f67-8a3b-c7f5e68445bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(all_dfs, ignore_index=True)\n",
    "df = df.astype({\"model\": \"category\", \"dataset\": \"category\"})\n",
    "df = df.sort_values([\"model\", \"dataset\", \"weight\", \"k\"])\n",
    "# df[\"val/acc_str\"] = \"$\" + (df[\"val/acc\"] * 100).round(2).astype(str) + \"\\%$\"\n",
    "\n",
    "df_orig = df[df[\"k\"] == 0]\n",
    "for i in range(len(df_orig)):\n",
    "    r = df_orig.iloc[i]\n",
    "    idx = (df[\"model\"] == r[\"model\"]) & (df[\"dataset\"] == r[\"dataset\"])\n",
    "    # gain = df.loc[idx, \"val/acc\"] / r[\"val/acc\"] - 1\n",
    "    gain = df.loc[idx, \"val/acc\"] - r[\"val/acc\"]\n",
    "    df.loc[idx, \"gain\"] = gain\n",
    "    # df.loc[idx, \"gain_str\"] = \"$\" + gain.round(2).astype(str) + \"\\%$\"\n",
    "\n",
    "# df = df[[\"model\", \"dataset\", \"weight\", \"weight_str\", \"k\", \"val/acc\", \"val/acc_str\", \"gain\", \"gain_str\"]]\n",
    "df = df[[\"model\", \"dataset\", \"weight\", \"k\", \"val/acc\", \"gain\"]]\n",
    "df = df[\n",
    "    df[\"dataset\"].isin(\n",
    "        [\n",
    "            \"cifar10\", \n",
    "            \"cifar100\", \n",
    "            \"mnist\", \n",
    "            \"fashionmnist\", \n",
    "            \"stl10\", \n",
    "            \"pcam\", \n",
    "            \"eurosat\", \n",
    "            \"semeion\"\n",
    "        ]\n",
    "    )\n",
    "]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01118590-b895-4b08-bc0c-9ec38ba2e473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def float_to_percent(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Converts a series of float e.g. `0.423` to a percentage LaTeX string `$42.3\\%$`\"\"\"\n",
    "    return \"$\" + (series * 100).round(2).astype(str) + \"\\%$\"\n",
    "\n",
    "def weight_to_tex(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Converts a series of powers of 10 to LaTeX\"\"\"\n",
    "    def _f(x: float) -> str:\n",
    "        if x == 1:\n",
    "            return \"$1$\"\n",
    "        e = int((np.log(x) / np.log(10)).round(1))\n",
    "        return \"$10^{\" + str(e) + \"}$\"\n",
    "    return series.apply(_f)\n",
    "\n",
    "real_column_names = {\n",
    "    \"model\": \"Model\", \n",
    "    \"dataset\": \"Dataset\", \n",
    "    \"weight\": \"$w$\", \n",
    "    \"k\": \"$k$\", \n",
    "    \"val/acc\": \"Acc.\", \n",
    "    \"gain\": \"Gain\",\n",
    "    \"tti\": \"FT/PT\",\n",
    "}\n",
    "real_model_names = {\n",
    "    \"alexnet\": \"AlexNet\",\n",
    "    \"resnet18\": \"ResNet-18\",\n",
    "    \"googlenet\": \"GoogLeNet\",\n",
    "    \"vgg16\": \"VGG-16\", \n",
    "}\n",
    "real_ds_names = {\n",
    "    \"cifar10\": \"CIFAR-10\",\n",
    "    \"cifar100\": \"CIFAR-100\",\n",
    "    \"mnist\": \"MNIST\",\n",
    "    \"fashionmnist\": \"Fashion-MNIST\",\n",
    "    \"stl10\": \"STL-10\",\n",
    "    \"pcam\": \"PCam\",\n",
    "    \"imagenet\": \"ImageNet\",\n",
    "    \"eurosat\": \"EuroSAT\",\n",
    "    \"semeion\": \"SEMEION\"\n",
    "}\n",
    "short_ds_names = {\n",
    "    \"cifar10\": \"CIF.10\",\n",
    "    \"cifar100\": \"CIF.100\",\n",
    "    \"mnist\": \"MNIST\",\n",
    "    \"fashionmnist\": \"Fashion\",\n",
    "    \"stl10\": \"STL10\",\n",
    "    \"pcam\": \"PCam\",\n",
    "    \"imagenet\": \"ImgNet\",\n",
    "    \"eurosat\": \"Eu.SAT\",\n",
    "    \"semeion\": \"SEM.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34b3568-581a-4fad-97cb-6feb93a9ebff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig = df[df[\"k\"] == 0][[\"model\", \"dataset\", \"val/acc\"]]\n",
    "df_orig.set_index([\"model\", \"dataset\"], inplace=True)\n",
    "\n",
    "df_orig_camera = df_orig.copy()\n",
    "df_orig_camera[\"val/acc\"] = float_to_percent(df_orig_camera[\"val/acc\"])\n",
    "df_orig_camera.rename(columns=real_column_names, inplace=True)\n",
    "df_orig_camera.index.names = [real_column_names[s] for s in df_orig_camera.index.names]\n",
    "df_orig_camera.rename(index=real_model_names, level=0, inplace=True)\n",
    "df_orig_camera.rename(index=real_ds_names, level=1, inplace=True)\n",
    "df_orig_camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10802dc-7e52-4d53-a970-581d7a7cc9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "idx = df[(df[\"k\"] != 0)].groupby([\"model\", \"dataset\"])[\"val/acc\"].idxmax().dropna()\n",
    "df_finetune_best = df.loc[idx][[\"model\", \"dataset\", \"weight\", \"k\", \"val/acc\", \"gain\"]]\n",
    "df_finetune_best.set_index([\"model\", \"dataset\"], inplace=True)\n",
    "\n",
    "df_finetune_best_camera = df_finetune_best.copy()\n",
    "# df_finetune_best_camera = df_finetune_best_camera.style.background_gradient(subset=\"gain\")\n",
    "df_finetune_best_camera[\"val/acc\"] = float_to_percent(df_finetune_best_camera[\"val/acc\"])\n",
    "df_finetune_best_camera[\"gain\"] = float_to_percent(df_finetune_best_camera[\"gain\"])\n",
    "df_finetune_best_camera[\"weight\"] = weight_to_tex(df_finetune_best_camera[\"weight\"])\n",
    "df_finetune_best_camera[\"k\"] = df_finetune_best_camera[\"k\"].apply(lambda s: f\"${s}$\")\n",
    "df_finetune_best_camera.rename(columns=real_column_names, inplace=True)\n",
    "df_finetune_best_camera.index.names = [real_column_names[s] for s in df_finetune_best_camera.index.names]\n",
    "df_finetune_best_camera.rename(index=real_model_names, level=0, inplace=True)\n",
    "df_finetune_best_camera.rename(index=real_ds_names, level=1, inplace=True)\n",
    "# df_finetune_best_camera.rename(index=short_ds_names, level=1, inplace=True)\n",
    "df_finetune_best_camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49edbc7d-0d3a-43c8-9e2f-aa981ff20672",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "df_hparm = pd.DataFrame(columns=[\"k\", \"weight\", \"avg_gain\"])\n",
    "\n",
    "for k, w in product(np.unique(df[\"k\"]), np.unique(df[\"weight\"])):\n",
    "    if k == 0 or w == 0:\n",
    "        continue\n",
    "    df_hparm.loc[len(df_hparm)] = {\n",
    "        \"k\": k,\n",
    "        \"weight\": w,\n",
    "        \"avg_gain\": df[(df[\"k\"] == k) & (df[\"weight\"] == w)][\"gain\"].mean(skipna=True)\n",
    "    }\n",
    "df_hparm.sort_values(\"avg_gain\", inplace=True, ascending=False)\n",
    "df_hparm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac10b923-8018-4aa4-a1a1-c6d983f7ab9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = (\n",
    "    df[\n",
    "        (df[\"gain\"] > 0) \n",
    "        & (df[\"k\"] == 5) \n",
    "        & (df[\"weight\"] == 1e-3)\n",
    "    ]\n",
    "    .groupby([\"model\", \"dataset\"])[\"val/acc\"]\n",
    "    .idxmax()\n",
    "    .dropna()\n",
    ")\n",
    "df_finetune_default = df.loc[idx][[\"model\", \"dataset\", \"val/acc\", \"gain\"]]\n",
    "df_finetune_default.set_index([\"model\", \"dataset\"], inplace=True)\n",
    "df_finetune_default[\"vs_best\"] = df_finetune_default[\"gain\"] - df_finetune_best[\"gain\"]\n",
    "\n",
    "df_finetune_default_camera = df_finetune_default.copy()\n",
    "df_finetune_default_camera[\"val/acc\"] = float_to_percent(df_finetune_default_camera[\"val/acc\"])\n",
    "df_finetune_default_camera[\"gain\"] = float_to_percent(df_finetune_default_camera[\"gain\"])\n",
    "df_finetune_default_camera[\"vs_best\"] = float_to_percent(df_finetune_default_camera[\"vs_best\"])\n",
    "df_finetune_default_camera.rename(columns=real_column_names, inplace=True)\n",
    "df_finetune_default_camera.rename(columns={\"vs_best\": \"Vs. best\"}, inplace=True)\n",
    "df_finetune_default_camera.index.names = [real_column_names[s] for s in df_finetune_default_camera.index.names]\n",
    "df_finetune_default_camera.rename(index=real_model_names, level=0, inplace=True)\n",
    "df_finetune_default_camera.rename(index=real_ds_names, level=1, inplace=True)\n",
    "# df_finetune_default_camera.rename(index=short_ds_names, level=1, inplace=True)\n",
    "\n",
    "df_finetune_default_camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecf4c5d-3eff-438d-8877-9de0ae0e7404",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wall = pd.read_csv(\"out/wall.csv\")\n",
    "df_wall.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "df_wall = df_wall.groupby([\"model\", \"dataset\", \"k\", \"weight\"]).mean()\n",
    "df_wall.drop([\"step\", \"epoch\"], axis=1, inplace=True)\n",
    "\n",
    "df_finetune_tti = df.copy()\n",
    "df_finetune_tti.reset_index(inplace=True)\n",
    "df_finetune_tti.set_index([\"model\", \"dataset\", \"k\", \"weight\"], inplace=True)\n",
    "# df_finetune_tti.drop([\"val/acc\", \"gain\"], axis=1, inplace=True)\n",
    "df_finetune_tti = df_finetune_tti.join(df_wall, how=\"inner\")\n",
    "df_finetune_tti.reset_index(inplace=True)\n",
    "df_finetune_tti = df_finetune_tti[df_finetune_tti[\"k\"] > 0]\n",
    "# df_finetune_tti.set_index([\"model\", \"dataset\"], inplace=True)\n",
    "\n",
    "df_wall.reset_index(inplace=True)\n",
    "df_wall_orig = df_wall[df_wall[\"k\"] == 0]\n",
    "for i in range(len(df_wall_orig)):\n",
    "    r = df_wall_orig.iloc[i]\n",
    "    idx = (\n",
    "        (df_finetune_tti[\"model\"] == r[\"model\"]) \n",
    "        & (df_finetune_tti[\"dataset\"] == r[\"dataset\"])\n",
    "    )\n",
    "    tti = df_finetune_tti.loc[idx, \"wall_time\"] / r[\"wall_time\"] - 1\n",
    "    df_finetune_tti.loc[idx, \"tti\"] = tti\n",
    "df_finetune_tti = df_finetune_tti[[\"model\", \"dataset\", \"tti\"]]\n",
    "df_finetune_tti = df_finetune_tti.groupby([\"model\", \"dataset\"]).mean()\n",
    "\n",
    "df_finetune_tti_camera = df_finetune_tti.copy()\n",
    "df_finetune_tti_camera[\"tti\"] = float_to_percent(df_finetune_tti_camera[\"tti\"])\n",
    "df_finetune_tti_camera.rename(columns=real_column_names, inplace=True)\n",
    "df_finetune_tti_camera.index.names = [real_column_names[s] for s in df_finetune_tti_camera.index.names]\n",
    "df_finetune_tti_camera.rename(index=real_model_names, level=0, inplace=True)\n",
    "df_finetune_tti_camera.rename(index=real_ds_names, level=1, inplace=True)\n",
    "# df_finetune_tti_camera.rename(index=short_ds_names, level=1, inplace=True)\n",
    "df_finetune_tti_camera\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4665906d-1332-448d-81b8-3d2ce33c3e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"multirow_align\": \"t\", \n",
    "    \"hrules\": True,  \n",
    "    \"position_float\": \"centering\"\n",
    "}\n",
    "table_size = \"tiny\"  # or \"small\"\n",
    "with open(\"out/val_acc_orig.tex\", \"w\", encoding=\"utf-8\") as fp:\n",
    "    latex = df_orig_camera.style.to_latex(\n",
    "        label=\"tab:val-acc:original\",\n",
    "        caption=\"Pretrained model accuracy.\",\n",
    "        position=\"H\",\n",
    "        **kwargs\n",
    "    )\n",
    "    latex = latex.replace(\n",
    "        \"\\\\begin{tabular}\", \n",
    "        f\"\\\\vskip 0.15in\\n\\\\{table_size}\\n\"\n",
    "        \"\\\\begin{tabular}\"\n",
    "    )\n",
    "    latex = latex.replace(\n",
    "        \"\\\\end{tabular}\", \n",
    "        \"\\\\end{tabular}\\n\\\\vskip -0.1in\"\n",
    "    )\n",
    "    fp.write(latex)\n",
    "with open(\"out/val_acc_finetune_best.tex\", \"w\", encoding=\"utf-8\") as fp:\n",
    "    latex = df_finetune_best_camera.style.to_latex(\n",
    "        label=\"tab:val-acc:finetune-best\",\n",
    "        caption=\"Best fine-tuning parameters and results.\",\n",
    "        position=\"t\",\n",
    "        **kwargs\n",
    "    )\n",
    "    latex = latex.replace(\n",
    "        \"\\\\begin{tabular}\", \n",
    "        (\n",
    "            f\"\\\\vskip 0.15in\\n\\\\{table_size}\\n\"\n",
    "            \"\\\\begin{tabular}\"\n",
    "        )\n",
    "    )\n",
    "    latex = latex.replace(\n",
    "        \"\\\\end{tabular}\", \n",
    "        \"\\\\end{tabular}\\n\\\\vskip -0.1in\"\n",
    "    )\n",
    "    fp.write(latex)\n",
    "with open(\"out/val_acc_finetune_default.tex\", \"w\", encoding=\"utf-8\") as fp:\n",
    "    latex = df_finetune_default_camera.style.to_latex(\n",
    "        label=\"tab:val-acc:finetune-default\",\n",
    "        caption=\"Best fine-tuning parameters and results for $k = 5$ and $w = 10^{-3}$.\",\n",
    "        position=\"H\",\n",
    "        **kwargs\n",
    "    )\n",
    "    latex = latex.replace(\n",
    "        \"\\\\begin{tabular}\", \n",
    "        (\n",
    "            f\"\\\\vskip 0.15in\\n\\\\{table_size}\\n\"\n",
    "            \"\\\\begin{tabular}\"\n",
    "        )\n",
    "    )\n",
    "    latex = latex.replace(\n",
    "        \"\\\\end{tabular}\", \n",
    "        \"\\\\end{tabular}\\n\\\\vskip -0.1in\"\n",
    "    )\n",
    "    fp.write(latex)\n",
    "with open(\"out/wall_finetune.tex\", \"w\", encoding=\"utf-8\") as fp:\n",
    "    latex = df_finetune_tti_camera.style.to_latex(\n",
    "        label=\"tab:wall:finetune-all\",\n",
    "        caption=(\n",
    "            \"Average ratio between fine-tuning time (FT) and pretraining time (PT). \"\n",
    "            \"For example, a FT/PT value of $100\\\\%$ means that fine-tuning took \"\n",
    "            \"as long as pretraining, on average, across all combinations of \"\n",
    "            \"parameters that were tested (see \\\\cref{ssec:setup}).\"\n",
    "        ),\n",
    "        position=\"t\",\n",
    "        **kwargs\n",
    "    )\n",
    "    latex = latex.replace(\n",
    "        \"\\\\begin{tabular}\", \n",
    "        (\n",
    "            f\"\\\\vskip 0.15in\\n\\\\{table_size}\\n\"\n",
    "            \"\\\\begin{tabular}\"\n",
    "        )\n",
    "    )\n",
    "    latex = latex.replace(\n",
    "        \"\\\\end{tabular}\", \n",
    "        \"\\\\end{tabular}\\n\\\\vskip -0.1in\"\n",
    "    )\n",
    "    fp.write(latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f9902d-c756-45de-887c-654760fdc54f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4af30a8c-e70a-4143-b479-fac250a22dd1",
   "metadata": {},
   "source": [
    "# Miss-match CE plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5043db02-59ed-471b-b874-757100b2f237",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"alexnet\"\n",
    "dataset_name = \"cifar10\"\n",
    "k, bs, we = 5, 2048, 3\n",
    "version = 0\n",
    "\n",
    "# experiment = model_name\n",
    "experiment = f\"{model_name}_finetune_l{k}_b{bs}_1e-{we}\"\n",
    "\n",
    "path = Path(\"out\") / experiment / dataset_name / f\"version_{version}\" / \"metrics.csv\"\n",
    "df = pd.read_csv(path)\n",
    "df = df[[\"epoch\", \"layer\", \"acc_all\", \"acc_miss\", \"acc_match\"]]\n",
    "df.rename(columns={\"acc_all\": \"all\", \"acc_miss\": \"miss\", \"acc_match\": \"match\"}, inplace=True)\n",
    "df = df.melt(\n",
    "    id_vars=[\"epoch\", \"layer\"],\n",
    "    var_name=\"subset\",\n",
    "    value_name=\"acc\",\n",
    ")\n",
    "\n",
    "real_subset_names = {\"all\": \"All\", \"match\": \"Correctly clust.\", \"miss\": \"Misscl.\"}\n",
    "df[\"subset\"] = df[\"subset\"].apply(lambda s: real_subset_names[s])\n",
    "df[\"layer\"] = df[\"layer\"].apply(lambda s: s[len(\"model.\"):])\n",
    "\n",
    "real_column_names = {\n",
    "    \"epoch\": \"Epoch\",\n",
    "    \"layer\": \"Layer\",\n",
    "    \"subset\": \"Sample group\",\n",
    "    \"acc\": \"Accuracy\",\n",
    "}\n",
    "df.rename(columns=real_column_names, inplace=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5729785a-e0d6-45e1-b989-76e798401a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = sns.relplot(\n",
    "    data=df,\n",
    "    x=real_column_names[\"epoch\"],\n",
    "    y=real_column_names[\"acc\"],\n",
    "    hue=real_column_names[\"subset\"],\n",
    "    palette={\n",
    "        real_subset_names[\"all\"]: \"black\", \n",
    "        real_subset_names[\"match\"]: \"blue\", \n",
    "        real_subset_names[\"miss\"]: \"red\"\n",
    "    },\n",
    "    style=real_column_names[\"subset\"],\n",
    "    dashes={\n",
    "        real_subset_names[\"all\"]: (1, 1),\n",
    "        real_subset_names[\"match\"]: \"\", \n",
    "        real_subset_names[\"miss\"]: \"\"\n",
    "    },\n",
    "    col=real_column_names[\"layer\"],\n",
    "    kind=\"line\",\n",
    "    col_wrap=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47974d77-0150-449a-b38f-becf83a67f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure.fig.savefig(f\"out/{experiment}_{dataset_name}_cluster_acc.png\")\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5781a3e6-bc82-4490-9515-024d4fb279d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
