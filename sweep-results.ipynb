{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be1212af-d6ba-4fbe-9244-0b8d45fcd0f4",
   "metadata": {},
   "source": [
    "Creates nice tables and stuff for the FITML'24 paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7f5231-f3fe-4251-a66f-69b80c42d463",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9631e77-f769-4dc7-97f9-be37339dbff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bokeh.layouts as bkl\n",
    "import bokeh.plotting as bk\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03eabed4-374b-4f63-9c4d-adb4d7b6b91d",
   "metadata": {},
   "source": [
    "# Load result files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb81e3ba-6e92-4cea-aa6d-ec4a41568347",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import turbo_broccoli as tb\n",
    "import regex as re\n",
    "from pathlib import Path\n",
    "\n",
    "SWEEP_DIRS = [\n",
    "    Path(\"out\") / \"sweep\",\n",
    "    Path(\"out\") / \"baselines\",\n",
    "]\n",
    "SWEEP_RESULTS = []\n",
    "\n",
    "for sweep_dir in SWEEP_DIRS:\n",
    "    for p in sweep_dir.glob(\"**/results.*.json\"):\n",
    "        if not re.match(r\"results\\.\\w+\\.json\", p.name):\n",
    "            continue\n",
    "        try:\n",
    "            print(\"Loading\", p)\n",
    "            data = tb.load(p)\n",
    "            SWEEP_RESULTS.append(data)\n",
    "        except Exception as e:\n",
    "            print(\"ERROR: Couldn't load\", p, \":\", type(e), str(e))\n",
    "\n",
    "print()\n",
    "print(\"Loaded\", len(SWEEP_RESULTS), \"results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bd75e1-05bf-4372-bfe3-27eafc1a1a77",
   "metadata": {},
   "source": [
    "# Gather baselines\n",
    "\n",
    "`baselines` is a dict\n",
    "\n",
    "    dataset -> model_name -> [\"acc1\" or \"acc5\"] -> a float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400a7dc0-21a0-45c0-bd18-ec257db3ed7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "BASELINE_OVERRIDES = {}\n",
    "\n",
    "baselines = defaultdict(dict)\n",
    "for r in SWEEP_RESULTS:\n",
    "    if r[\"model\"][\"hparams\"][\"lcc_kwargs\"] is not None:\n",
    "        continue\n",
    "    dataset, model = r[\"dataset\"][\"name\"], r[\"model\"][\"name\"]\n",
    "    if model not in baselines[dataset]:\n",
    "        baselines[dataset][model] = {\"acc1\": [], \"acc5\": []}\n",
    "    baselines[dataset][model][\"acc1\"].append(r[\"training\"][\"test\"][0][\"test/acc\"])\n",
    "    baselines[dataset][model][\"acc5\"].append(r[\"training\"][\"test\"][0][\"test/acc5\"])\n",
    "\n",
    "for bls in baselines.values():\n",
    "    for model, d in bls.items():\n",
    "        bls[model][\"acc1\"] = np.max(bls[model][\"acc1\"])\n",
    "        bls[model][\"acc5\"] = np.max(bls[model][\"acc5\"])\n",
    "\n",
    "baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a15d07-e21c-4563-a7a5-ba6fd37a71ac",
   "metadata": {},
   "source": [
    "# Gather all results (incl. baselines) in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2c3854-f5b5-45d5-aed1-1d0eb287e221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sweep import LCC_INTERVALS, LCC_WARMUPS, LCC_WEIGHTS, LCC_KS\n",
    "from sweep import MODELS as _MODELS\n",
    "from sweep import DATASETS as _DATASETS\n",
    "\n",
    "MODELS = set(d[\"name\"] for d in _MODELS)\n",
    "DATASETS = set(d[\"name\"] for d in _DATASETS)\n",
    "\n",
    "GAIN_THRESHOLD = -10  # in percent\n",
    "\n",
    "data = []\n",
    "\n",
    "for ds, bsls in baselines.items():\n",
    "    if ds not in DATASETS:\n",
    "        continue\n",
    "    for model, d in bsls.items():\n",
    "        if model not in MODELS:\n",
    "            continue\n",
    "        data.append(\n",
    "            {\n",
    "                \"model\": model,\n",
    "                \"dataset\": ds,\n",
    "                \"test_acc1\": d[\"acc1\"],\n",
    "                \"test_acc5\": d[\"acc5\"],\n",
    "                # \"gain1\": 0,\n",
    "                # \"gain5\": 0,\n",
    "            }\n",
    "        )\n",
    "\n",
    "for r in SWEEP_RESULTS:\n",
    "    if r[\"model\"][\"name\"] not in MODELS or r[\"dataset\"][\"name\"] not in DATASETS:\n",
    "        # print(\n",
    "        #     \"Run not in model list or dataset list:\",\n",
    "        #     r[\"model\"][\"name\"],\n",
    "        #     r[\"dataset\"][\"name\"],\n",
    "        # )\n",
    "        continue\n",
    "    lcc_kwargs = r[\"model\"][\"hparams\"][\"lcc_kwargs\"]\n",
    "    lcc_submodules = r[\"model\"][\"hparams\"].get(\"lcc_submodules\")\n",
    "    if lcc_submodules is None:  # Baseline run\n",
    "        continue\n",
    "    if not (\n",
    "        lcc_kwargs is None\n",
    "        or (\n",
    "            lcc_kwargs[\"weight\"] in LCC_WEIGHTS\n",
    "            and lcc_kwargs[\"interval\"] in LCC_INTERVALS\n",
    "            and lcc_kwargs[\"warmup\"] in LCC_WARMUPS\n",
    "            and lcc_kwargs[\"k\"] in LCC_KS\n",
    "        )\n",
    "    ):\n",
    "        # print(\"Run LCC parameters now in sweep\")\n",
    "        continue\n",
    "    if hasattr(lcc_submodules, \"__len__\") and len(lcc_submodules) > 1:\n",
    "        # print(\"Run has more than one LCC submodule\")\n",
    "        continue\n",
    "\n",
    "    row = {\n",
    "        \"model\": r[\"model\"][\"name\"],\n",
    "        \"dataset\": r[\"dataset\"][\"name\"],\n",
    "        \"test_acc1\": r[\"training\"][\"test\"][0][\"test/acc\"],\n",
    "    }\n",
    "    if (a5 := r[\"training\"][\"test\"][0].get(\"test/acc5\")) is not None:\n",
    "        row[\"test_acc5\"] = a5\n",
    "    if lcc_kwargs is None:\n",
    "        row[\"gain1\"], row[\"gain5\"] = 0, 0\n",
    "    else:\n",
    "        row[\"submodule\"] = \",\".join(lcc_submodules)\n",
    "        row.update(lcc_kwargs)\n",
    "        try:\n",
    "            bacc1 = baselines[row[\"dataset\"]][row[\"model\"]][\"acc1\"]\n",
    "            gain = row[\"test_acc1\"] - bacc1\n",
    "            row[\"gain1\"] = gain\n",
    "        except KeyError:\n",
    "            row[\"gain1\"] = 0\n",
    "        try:\n",
    "            bacc5 = baselines[row[\"dataset\"]][row[\"model\"]][\"acc5\"]\n",
    "            gain = row[\"test_acc5\"] - bacc5\n",
    "            row[\"gain5\"] = gain\n",
    "        except KeyError:\n",
    "            row[\"gain5\"] = 0\n",
    "\n",
    "    data.append(row)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "# df = df.sort_values(columns, na_position=\"first\")\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3206fce6-cb37-4694-8e19-acec2d9d3b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"model\", \"dataset\", \"weight\", \"k\", \"interval\", \"warmup\", \"submodule\"]\n",
    "columns = [s for s in columns if s in df.columns]\n",
    "df = df.groupby(columns, dropna=False).mean()\n",
    "df = df.reset_index()\n",
    "df = df.sort_values(columns, na_position=\"first\")\n",
    "\n",
    "interesting_columns = [\n",
    "    c\n",
    "    for c in df.columns\n",
    "    if (df[c].isna().any() and len(df[c].unique()) > 2)\n",
    "    or (not df[c].isna().any() and len(df[c].unique()) > 1)\n",
    "]\n",
    "if nic := [c for c in df.columns if c not in interesting_columns]:\n",
    "    print(\"Dropping\", nic)\n",
    "    df = df.drop(nic, axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b4619a-6043-468e-9216-28b488070209",
   "metadata": {},
   "source": [
    "# Tex export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08255ef-c0d9-4952-9152-ffad86b1d53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_DECIMALS = 2\n",
    "\n",
    "\n",
    "def float_to_latex(x: float | str) -> str:\n",
    "    if isinstance(x, str):\n",
    "        return x\n",
    "    if np.isnan(x):\n",
    "        return \"\"\n",
    "    float_str = f\"{x:.2E}\"\n",
    "    if \"E\" in float_str:\n",
    "        base, exponent = float_str.split(\"E\")\n",
    "        base, exponent = int(float(base)), int(exponent)\n",
    "        if exponent < 0:\n",
    "            if base == 1:\n",
    "                return f\"$10^{{{exponent}}}$\"\n",
    "            return f\"${base} \\\\times 10^{{{exponent}}}$\"\n",
    "        return \"$\" + str(int(base * 10**exponent)) + \"$\"\n",
    "    return \"$\" + str(round(x, N_DECIMALS)) + \"$\"\n",
    "\n",
    "\n",
    "def int_to_latex(x: float | str) -> str:\n",
    "    if isinstance(x, str):\n",
    "        return x\n",
    "    if np.isnan(x):\n",
    "        return \"\"\n",
    "    return \"$\" + str(int(x)) + \"$\"\n",
    "\n",
    "\n",
    "def percent_to_latex(x: float | str, sign: bool = False) -> str:\n",
    "    if isinstance(x, str):\n",
    "        return x\n",
    "    if np.isnan(x):\n",
    "        return \"\"\n",
    "    return (\n",
    "        \"$\" + (\"+\" if sign and x > 0 else \"\") + str(round(x * 100, N_DECIMALS)) + \"\\\\%$\"\n",
    "    )\n",
    "\n",
    "\n",
    "data = []\n",
    "for _, row in df.iterrows():\n",
    "\n",
    "    row_fmt = {}\n",
    "\n",
    "    if dataset := row.get(\"dataset\"):\n",
    "        if dataset == \"cifar100\":\n",
    "            dataset = \"CIFAR100\"\n",
    "        elif dataset == \"timm/imagenet-1k-wds\":\n",
    "            dataset = \"ImageNet\"\n",
    "        else:\n",
    "            print(\"Unknown dataset:\", dataset, \"Skipping.\")\n",
    "            continue\n",
    "        row_fmt[\"dataset\"] = dataset\n",
    "\n",
    "    if model := row.get(\"model\"):\n",
    "        model = row[\"model\"]\n",
    "        if model.startswith(\"google/mobilenet\") or model.startswith(\"timm/mobilenet\"):\n",
    "            model = \"MobileNet\"\n",
    "        elif model.startswith(\"microsoft/resnet-18\") or model.startswith(\n",
    "            \"timm/resnet18\"\n",
    "        ):\n",
    "            model = \"ResNet18\"\n",
    "        elif model.startswith(\"google/vit-base-patch16-224\"):\n",
    "            model = \"ViT\"\n",
    "        elif model.startswith(\"timm/tinynet\"):\n",
    "            model = \"TinyNet\"\n",
    "        elif model.startswith(\"timm/vgg11\"):\n",
    "            model = \"VGG11\"\n",
    "        elif model.startswith(\"timm/tf_efficientnet_l2\"):\n",
    "            model = \"EfficientNet\"\n",
    "        elif model.startswith(\"timm/convnext_small\"):\n",
    "            model = \"ConvNeXt\"\n",
    "        elif model.startswith(\"timm/inception_v3\"):\n",
    "            model = \"InceptionV3\"\n",
    "        elif model == \"alexnet\":\n",
    "            model = \"AlexNet\"\n",
    "        else:\n",
    "            print(\"Unknown model:\", model, \"Skipping.\")\n",
    "            continue\n",
    "        row_fmt[\"Model\"] = model\n",
    "\n",
    "    layer = row.get(\"submodule\")\n",
    "    if layer is None or not isinstance(layer, str):\n",
    "        layer = \"\"\n",
    "    elif layer in [\"classifier\", \"head\", \"fc\"]:\n",
    "        layer = \"Head\"\n",
    "    else:\n",
    "        layer = \"$2^{\\\\text{nd}}$ to last\"\n",
    "        # layer = \"$\\\\texttt{\" + str(layer).replace(\"_\", \"\\_\") + \"}$\"\n",
    "    row_fmt[\"Layer\"] = layer\n",
    "\n",
    "    if weight := row.get(\"weight\"):\n",
    "        row_fmt[\"$w$\"] = float_to_latex(weight)\n",
    "\n",
    "    if k := row.get(\"k\"):\n",
    "        row_fmt[\"$k$\"] = int_to_latex(k)\n",
    "\n",
    "    if (wmp := row.get(\"warmup\")) is not None:\n",
    "        row_fmt[\"Wmp.\"] = int_to_latex(row[\"warmup\"])\n",
    "\n",
    "    if interval := row.get(\"interval\"):\n",
    "        row_fmt[\"Int.\"] = int_to_latex(row[\"interval\"])\n",
    "\n",
    "    row_fmt.update(\n",
    "        {\n",
    "            \"Acc. (top 1)\": percent_to_latex(row[\"test_acc1\"], sign=False),\n",
    "            \"Gain (top 1)\": (\n",
    "                percent_to_latex(row[\"gain1\"], sign=True) if row[\"gain1\"] else \"\"\n",
    "            ),\n",
    "            \"Acc. (top 5)\": percent_to_latex(row[\"test_acc5\"], sign=False),\n",
    "            \"Gain (top 5)\": (\n",
    "                percent_to_latex(row[\"gain5\"], sign=True) if row[\"gain5\"] else \"\"\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "    data.append(row_fmt)\n",
    "\n",
    "df_tex = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3093424-0a6a-49c7-bfde-32dc67d79aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n",
    "    display(df_tex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233d589e-daf7-4124-8fc0-eea16c7b2dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "OUTPUT_PATH = Path(\"out/papers/\")\n",
    "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86aeeab-3778-41b4-af8b-42c58470dcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with (OUTPUT_PATH / \"results-all.tex\").open(\"w\", encoding=\"utf-8\") as fp:\n",
    "    df_tex.to_latex(fp, index=False)\n",
    "\n",
    "for model in df_tex[\"Model\"].unique():\n",
    "    with (OUTPUT_PATH / f\"results-{model.lower()}.tex\").open(\n",
    "        \"w\", encoding=\"utf-8\"\n",
    "    ) as fp:\n",
    "        a = df_tex[df_tex[\"Model\"] == model]\n",
    "        a.drop(\"Model\", axis=1).to_latex(fp, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8522451-3431-4b60-b160-8e64cee2cd82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ebff08af-c886-400e-adab-b8fc52e334e2",
   "metadata": {},
   "source": [
    "# Typst export\n",
    "\n",
    "Reference: https://typst.app/docs/reference/model/table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a55650a-f2d4-494c-9039-85ea9cf2f11d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "header = [f\"[*{c}*]\" for c in data[0]]\n",
    "n_columns = len(header)\n",
    "\n",
    "typst = f\"\"\"\n",
    "#figure(table(\n",
    "  columns: ({', '.join(['auto'] * n_columns)}),\n",
    "  inset: 15pt,\n",
    "  align: horizon,\n",
    "  table.header(\n",
    "    {', '.join(header)}\n",
    "  ),\"\"\"\n",
    "\n",
    "for d in data:\n",
    "    typst += \"\\n// ------------------------------\"\n",
    "    for k, v in d.items():\n",
    "        if m := re.match(r\"\\$10\\^{(.*)}\\$\", v):\n",
    "            v = f\"$10^({m.group(1)})$\"\n",
    "        elif m := re.match(r\"\\$(.*)\\^{\\\\text{(.*)}}\\$(.*)\", v):\n",
    "            a, b, c = m.groups()\n",
    "            v = f'${a}^(\"{b}\")${c}'\n",
    "        v = v.replace(\"\\\\%\", \"%\")\n",
    "        typst += f\"\\n  [{v}], // {k}\"\n",
    "\n",
    "typst += \"\\n))\"\n",
    "print(typst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921e4475-3661-4dab-a547-39305c376bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with (OUTPUT_PATH / \"results-all.typ\").open(\"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(typst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff244f5-1bbf-488f-94bf-9e8907afa0a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "132248c1-6988-48af-9d52-631b2498891d",
   "metadata": {},
   "source": [
    "# Converting result JSON schema"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fb251779-da8d-41cd-acfc-f967dc16c1e4",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "import turbo_broccoli as tb\n",
    "from sweep import _hash_dict\n",
    "\n",
    "SWEEP_DIR = Path(\"out\") / \"sweep\"\n",
    "\n",
    "for p in (SWEEP_DIR / \"old\").glob(\"*.done\"):\n",
    "    print()\n",
    "    print(\"Loading\", p.name)\n",
    "    data = tb.load(p)\n",
    "    data[\"conf\"][\"seed\"] = 0\n",
    "    # print(data)\n",
    "    h = _hash_dict(data[\"conf\"])\n",
    "    print(p.name, \"new hash ->\", h)\n",
    "    tb.save(data, SWEEP_DIR / (h + \".done\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b47f96-7ab6-4b08-bc0b-63adcd342eaa",
   "metadata": {},
   "source": [
    "# Accuracy top 5"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fa913d39-f0fa-48bd-b9af-f24ea0a3c4c5",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from pathlib import Path\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "import turbo_broccoli as tb\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from loguru import logger as logging\n",
    "\n",
    "from lcc.classifiers import get_classifier_cls\n",
    "from lcc.datasets import HuggingFaceDataset\n",
    "from lcc.training import make_trainer\n",
    "\n",
    "SWEEP_DIR = Path(\"out\") / \"baselines\"\n",
    "\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "for p in SWEEP_DIR.glob(\"**/results.*.json\"):\n",
    "    logging.info(\"Loading results {}\", p)\n",
    "    data = tb.load(p)\n",
    "    if \"test/acc5\" in data[\"training\"][\"test\"][0]:\n",
    "        logging.info(\"Results {} already has test/acc5, skipping\", p)\n",
    "        continue\n",
    "\n",
    "    model_name = data[\"model\"][\"name\"]\n",
    "    cls = get_classifier_cls(model_name)\n",
    "    ckpt = data[\"training\"][\"best_checkpoint\"][\"path\"]\n",
    "    ckpt = SWEEP_DIR / ckpt\n",
    "    model = cls.load_from_checkpoint(ckpt)\n",
    "\n",
    "    ds = HuggingFaceDataset(\n",
    "        dataset_name=data[\"dataset\"][\"name\"],\n",
    "        fit_split=data[\"dataset\"][\"train_split\"],  # WTF IS THIS SHIT\n",
    "        val_split=data[\"dataset\"][\"val_split\"],\n",
    "        test_split=data[\"dataset\"][\"test_split\"],\n",
    "        image_processor=cls.get_image_processor(model_name),\n",
    "        label_key=data[\"dataset\"][\"label_key\"],\n",
    "        train_dl_kwargs={\n",
    "            \"batch_size\": data[\"dataset\"][\"batch_size\"],\n",
    "            \"num_workers\": 4,\n",
    "        },\n",
    "        val_dl_kwargs={\n",
    "            \"batch_size\": data[\"dataset\"][\"batch_size\"],\n",
    "            \"num_workers\": 4,\n",
    "        },\n",
    "        test_dl_kwargs={\n",
    "            \"batch_size\": data[\"dataset\"][\"batch_size\"],\n",
    "            \"num_workers\": 4,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    with TemporaryDirectory() as output_dir:\n",
    "        trainer = make_trainer(output_dir, model_name, stage=\"test\")\n",
    "        results = trainer.test(model, ds)\n",
    "\n",
    "    a, b = data[\"training\"][\"test\"][0][\"test/acc\"], results[0][\"test/acc\"]\n",
    "    if not np.isclose(a, b):\n",
    "        logging.warning(\"Old/new test/acc aren't close: {} vs. {}\", a, b)\n",
    "\n",
    "    data[\"training\"][\"test\"][0][\"test/acc5\"] = results[0][\"test/acc5\"]\n",
    "    tb.save(data, p)\n",
    "    logging.info(\"Updated {}\", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1872ed58-1820-458e-8635-f0229ab33246",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlnas",
   "language": "python",
   "name": "nlnas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
